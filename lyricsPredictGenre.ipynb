{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark import SQLContext, SparkContext\n",
    "from collections import namedtuple\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType, Row, ArrayType, StringType\n",
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.mllib.feature import HashingTF, IDF\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, NaiveBayes, DecisionTreeClassifier\n",
    "\n",
    "from pyspark.ml.feature import  HashingTF,Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"COMP 4651\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+-----+--------------------+--------------------+----+\n",
      "|         artist|genre|index|              lyrics|                song|year|\n",
      "+---------------+-----+-----+--------------------+--------------------+----+\n",
      "|beyonce-knowles|  Pop|    0|Oh baby, how you ...|           ego-remix|2009|\n",
      "|beyonce-knowles|  Pop|    1|playin' everythin...|        then-tell-me|2009|\n",
      "|beyonce-knowles|  Pop|    2|If you search\n",
      "For...|             honesty|2009|\n",
      "|beyonce-knowles|  Pop|    3|Oh oh oh I, oh oh...|     you-are-my-rock|2009|\n",
      "|beyonce-knowles|  Pop|    4|Party the people,...|       black-culture|2009|\n",
      "|beyonce-knowles|  Pop|    5|I heard\n",
      "Church be...|all-i-could-do-wa...|2009|\n",
      "|beyonce-knowles|  Pop|    6|This is just anot...|  once-in-a-lifetime|2009|\n",
      "|beyonce-knowles|  Pop|    7|Waiting, waiting,...|             waiting|2009|\n",
      "|beyonce-knowles|  Pop|    8|[Verse 1:]\n",
      "I read...|           slow-love|2009|\n",
      "|beyonce-knowles|  Pop|    9|N-n-now, honey\n",
      "Yo...|why-don-t-you-lov...|2009|\n",
      "|beyonce-knowles|  Pop|   10|I lay alone awake...|       save-the-hero|2009|\n",
      "|beyonce-knowles|  Pop|   11|Hello hello baby ...|           telephone|2009|\n",
      "|beyonce-knowles|  Pop|   12|Feels like I'm lo...|     ice-cream-truck|2009|\n",
      "|beyonce-knowles|  Pop|   13|Youre everything ...|no-broken-hearted...|2009|\n",
      "|beyonce-knowles|  Pop|   14|I gotta give up\n",
      "t...|             control|2009|\n",
      "|beyonce-knowles|  Pop|   15|It really hurts t...|       i-m-alone-now|2009|\n",
      "|beyonce-knowles|  Pop|   16|You're bad for me...|              poison|2009|\n",
      "|beyonce-knowles|  Pop|   17|[Chorus:]\n",
      "I'm a w...|    world-wide-women|2007|\n",
      "|beyonce-knowles|  Pop|   18|Ay\n",
      "Ay\n",
      "Ay (Nobody ...|      beautiful-liar|2007|\n",
      "|beyonce-knowles|  Pop|   19|Ay! Ay!\n",
      "(Nobody l...|beautiful-liar-sp...|2007|\n",
      "+---------------+-----+-----+--------------------+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sqlContext = SQLContext(sc)\n",
    "\n",
    "\n",
    "\n",
    "#compute the length of song\n",
    "size_ = udf(lambda xs: len(xs), IntegerType())\n",
    "\n",
    "#filter out null song value  note: filter == where, dunno why do they create two functions...\n",
    "raw_df = spark.read.json(\"data/lyrics.json\").where( (col('lyrics')).isNotNull() ).filter(size_(col('lyrics')) >0)\n",
    "\n",
    "# .na.drop(subset=[\"\"])\n",
    "# raw_df.select(raw_df['lyrics'], F.where((raw_df['index'] == 160))).show()\n",
    "raw_df\n",
    "raw_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+-----+--------------------+--------------------+----+-----+\n",
      "|         artist|genre|index|              lyrics|                song|year|label|\n",
      "+---------------+-----+-----+--------------------+--------------------+----+-----+\n",
      "|beyonce-knowles|  Pop|    0|Oh baby, how you ...|           ego-remix|2009|    6|\n",
      "|beyonce-knowles|  Pop|    1|playin' everythin...|        then-tell-me|2009|    6|\n",
      "|beyonce-knowles|  Pop|    2|If you search\n",
      "For...|             honesty|2009|    6|\n",
      "|beyonce-knowles|  Pop|    3|Oh oh oh I, oh oh...|     you-are-my-rock|2009|    6|\n",
      "|beyonce-knowles|  Pop|    4|Party the people,...|       black-culture|2009|    6|\n",
      "|beyonce-knowles|  Pop|    5|I heard\n",
      "Church be...|all-i-could-do-wa...|2009|    6|\n",
      "|beyonce-knowles|  Pop|    6|This is just anot...|  once-in-a-lifetime|2009|    6|\n",
      "|beyonce-knowles|  Pop|    7|Waiting, waiting,...|             waiting|2009|    6|\n",
      "|beyonce-knowles|  Pop|    8|[Verse 1:]\n",
      "I read...|           slow-love|2009|    6|\n",
      "|beyonce-knowles|  Pop|    9|N-n-now, honey\n",
      "Yo...|why-don-t-you-lov...|2009|    6|\n",
      "|beyonce-knowles|  Pop|   10|I lay alone awake...|       save-the-hero|2009|    6|\n",
      "|beyonce-knowles|  Pop|   11|Hello hello baby ...|           telephone|2009|    6|\n",
      "|beyonce-knowles|  Pop|   12|Feels like I'm lo...|     ice-cream-truck|2009|    6|\n",
      "|beyonce-knowles|  Pop|   13|Youre everything ...|no-broken-hearted...|2009|    6|\n",
      "|beyonce-knowles|  Pop|   14|I gotta give up\n",
      "t...|             control|2009|    6|\n",
      "|beyonce-knowles|  Pop|   15|It really hurts t...|       i-m-alone-now|2009|    6|\n",
      "|beyonce-knowles|  Pop|   16|You're bad for me...|              poison|2009|    6|\n",
      "|beyonce-knowles|  Pop|   17|[Chorus:]\n",
      "I'm a w...|    world-wide-women|2007|    6|\n",
      "|beyonce-knowles|  Pop|   18|Ay\n",
      "Ay\n",
      "Ay (Nobody ...|      beautiful-liar|2007|    6|\n",
      "|beyonce-knowles|  Pop|   19|Ay! Ay!\n",
      "(Nobody l...|beautiful-liar-sp...|2007|    6|\n",
      "+---------------+-----+-----+--------------------+--------------------+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# change genre to numeric\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "\n",
    "\n",
    "genreList = raw_df.select('genre').distinct().rdd.map(lambda x:x.genre).collect()\n",
    "genreList\n",
    "\n",
    "def changeToNumeric(genre):\n",
    "    return genreList.index(genre)\n",
    "\n",
    "udf_changeToNumeric = udf(changeToNumeric, IntegerType()) # if the function returns an int\n",
    "raw_df = raw_df.withColumn(\"label\", udf_changeToNumeric('genre'))\n",
    "raw_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/data/khuangaf/miniconda2/envs/py27/lib/python2.7/site-packages/py4j/java_gateway.py\", line 883, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/data/khuangaf/miniconda2/envs/py27/lib/python2.7/site-packages/py4j/java_gateway.py\", line 1040, in send_command\n",
      "    \"Error while receiving\", e, proto.ERROR_ON_RECEIVE)\n",
      "Py4JNetworkError: Error while receiving\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 59638)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/data/khuangaf/miniconda2/envs/py27/lib/python2.7/SocketServer.py\", line 290, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/home/data/khuangaf/miniconda2/envs/py27/lib/python2.7/SocketServer.py\", line 318, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/home/data/khuangaf/miniconda2/envs/py27/lib/python2.7/SocketServer.py\", line 331, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/home/data/khuangaf/miniconda2/envs/py27/lib/python2.7/SocketServer.py\", line 652, in __init__\n",
      "    self.handle()\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:39947)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/data/khuangaf/miniconda2/envs/py27/lib/python2.7/site-packages/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "  File \"/home/data/khuangaf/miniconda2/envs/py27/lib/python2.7/socket.py\", line 228, in meth\n",
      "    return getattr(self._sock,name)(*args)\n",
      "error: [Errno 111] Connection refused\n",
      "  File \"/home/data/khuangaf/miniconda2/envs/py27/lib/python2.7/site-packages/pyspark/accumulators.py\", line 235, in handle\n",
      "    num_updates = read_int(self.rfile)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/data/khuangaf/miniconda2/envs/py27/lib/python2.7/site-packages/pyspark/serializers.py\", line 577, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n"
     ]
    },
    {
     "ename": "Py4JNetworkError",
     "evalue": "An error occurred while trying to connect to the Java server (127.0.0.1:39947)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JNetworkError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/home/data/khuangaf/miniconda2/envs/py27/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_code\u001b[0;34m(self, code_obj, result)\u001b[0m\n\u001b[1;32m   2897\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2898\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_in_exec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2899\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2900\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m             \u001b[0moutflag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/data/khuangaf/miniconda2/envs/py27/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only)\u001b[0m\n\u001b[1;32m   1826\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   1827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1828\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1829\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_pdb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1830\u001b[0m                         \u001b[0;31m# drop into debugger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/data/khuangaf/miniconda2/envs/py27/lib/python2.7/site-packages/ipykernel/zmqshell.pyc\u001b[0m in \u001b[0;36m_showtraceback\u001b[0;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[1;32m    541\u001b[0m             \u001b[0;34mu'traceback'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             \u001b[0;34mu'ename'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0municode_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m             \u001b[0;34mu'evalue'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mpy3compat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msafe_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m         }\n\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/data/khuangaf/miniconda2/envs/py27/lib/python2.7/site-packages/ipython_genutils/py3compat.pyc\u001b[0m in \u001b[0;36msafe_unicode\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \"\"\"\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0municode_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mUnicodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/data/khuangaf/miniconda2/envs/py27/lib/python2.7/site-packages/py4j/protocol.pyc\u001b[0m in \u001b[0;36m__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0mgateway_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception_cmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m         \u001b[0mreturn_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_return_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"{0}: {1}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/data/khuangaf/miniconda2/envs/py27/lib/python2.7/site-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    879\u001b[0m          \u001b[0;32mif\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m         \"\"\"\n\u001b[0;32m--> 881\u001b[0;31m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/data/khuangaf/miniconda2/envs/py27/lib/python2.7/site-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    827\u001b[0m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    830\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/data/khuangaf/miniconda2/envs/py27/lib/python2.7/site-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m_create_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    833\u001b[0m         connection = GatewayConnection(\n\u001b[1;32m    834\u001b[0m             self.gateway_parameters, self.gateway_property)\n\u001b[0;32m--> 835\u001b[0;31m         \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/data/khuangaf/miniconda2/envs/py27/lib/python2.7/site-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    968\u001b[0m                 \u001b[0;34m\"server ({0}:{1})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPy4JNetworkError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JNetworkError\u001b[0m: An error occurred while trying to connect to the Java server (127.0.0.1:39947)"
     ]
    }
   ],
   "source": [
    "# Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n",
    "tokenizer = Tokenizer(inputCol=\"lyrics\", outputCol=\"tokenized_song\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\",numFeatures = 500)\n",
    "\n",
    "# lr = LogisticRegression(maxIter=100, regParam=0.001)\n",
    "# lr = RandomForestClassifier( numTrees=30)\n",
    "\n",
    "lr = DecisionTreeClassifier()\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "raw_df = raw_df.select(col(\"index\"), col(\"lyrics\"), col('label'))\n",
    "# Fit the pipeline to training documents.\n",
    "\n",
    "#sample without replacement\n",
    "training_df = raw_df.sample(False,0.7)\n",
    "model = pipeline.fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "\n",
    "#sample without replacement\n",
    "validation_df = raw_df.sample(False,0.1)\n",
    "prediction = model.transform(validation_df)\n",
    "selected = prediction.select(\"index\", \"prediction\")\n",
    "combined_df = validation_df.select('index','label').join(selected, validation_df.index == selected.index)\n",
    "\n",
    "# for row in combined_df.collect():\n",
    "#     rid, prob, prediction = row\n",
    "#     print(\"(%d) --> prob=%s, prediction=%f\" % (rid, str(prob), prediction))\n",
    "    \n",
    "num_correct = combined_df.select( (combined_df.label == combined_df.prediction).alias('correct') ).rdd.map(lambda x: 1 if x.correct == True else 0).reduce(lambda a,b:a+b)\n",
    "accuracy = (num_correct + 0.0) / combined_df.count()\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+-----+--------------------+---------------+----+-------------+\n",
      "|         artist|genre|index|              lyrics|           song|year|numeric_genre|\n",
      "+---------------+-----+-----+--------------------+---------------+----+-------------+\n",
      "|beyonce-knowles|  Pop|    0|Oh baby, how you ...|      ego-remix|2009|            6|\n",
      "|beyonce-knowles|  Pop|    1|playin' everythin...|   then-tell-me|2009|            6|\n",
      "|beyonce-knowles|  Pop|    2|If you search\n",
      "For...|        honesty|2009|            6|\n",
      "|beyonce-knowles|  Pop|    3|Oh oh oh I, oh oh...|you-are-my-rock|2009|            6|\n",
      "+---------------+-----+-----+--------------------+---------------+----+-------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# split_regex = r'\\W+'\n",
    "# STOPWORDS_PATH = 'data/stopwords.txt'\n",
    "# stopwords = set(spark.read.text(STOPWORDS_PATH).rdd.map(lambda x: x.value).collect())\n",
    "# def tokenize(string):\n",
    "#     \"\"\" An implementation of input string tokenization that excludes stopwords\n",
    "#     Args:\n",
    "#         string (str): input string\n",
    "#     Returns:\n",
    "#         list: a list of tokens without stopwords\n",
    "#     \"\"\"\n",
    "    \n",
    "#     regexed = [ token.lower() for token in re.split(split_regex, string) if len(token)]\n",
    "#     return  [token for token in regexed if token not in stopwords]#get rid of empty stuff\n",
    "# udf_tokenize = udf(tokenize, ArrayType(StringType())) # if the function returns an int\n",
    "# raw_df = raw_df.withColumn(\"tokenized_song\", udf_changeToNumeric('lyrics'))\n",
    "# raw_df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# # get rid of special character and split, then tokenize\n",
    "# raw_to_token = raw_df.select('index','lyrics').rdd.map(lambda x: (x[0],re.sub(r\"[^a-zA-Z0-9]+\", ' ', x[1]).split(\" \") ))\\\n",
    "# .map(lambda x: (x[0],  tokenize(' '.join(x[1])) ))\n",
    "# raw_to_token.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# hashingTF = HashingTF()\n",
    "# tf = hashingTF.transform()\n",
    "\n",
    "# # While applying HashingTF only needs a single pass to the data, applying IDF needs two passes:\n",
    "# # First to compute the IDF vector and second to scale the term frequencies by IDF.\n",
    "# tf.cache()\n",
    "# idf = IDF().fit(tf)\n",
    "# tfidf = idf.transform(tf)\n",
    "# #\n",
    "# # spark.mllib's IDF implementation provides an option for ignoring terms\n",
    "# # which occur in less than a minimum number of documents.\n",
    "# # In such cases, the IDF for these terms is set to 0.\n",
    "# # This feature can be used by passing the minDocFreq value to the IDF constructor.\n",
    "# idfIgnore = IDF(minDocFreq=2).fit(tf)\n",
    "# tfidfIgnore = idfIgnore.transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "\n",
    "combinedDf.cache()\n",
    "parsedData = combinedDf.rdd.sample(False, 0.01).map(lambda x: LabeledPoint (x[2], x[1]))\n",
    "\n",
    "# Build the model\n",
    "model = LogisticRegressionWithLBFGS.train(parsedData)\n",
    "\n",
    "# # Evaluating the model on training data\n",
    "# labelsAndPreds = parsedData.map(lambda p: (p.label, model.predict(p.features)))\n",
    "# trainErr = labelsAndPreds.filter(lambda lp: lp[0] != lp[1]).count() / float(parsedData.count())\n",
    "# print(\"Training Error = \" + str(trainErr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# spark.read.parquet('tfidf.parquet').toDF('index','song').createOrReplaceTempView('tfidfTable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tfidfDf = spark.read.table('tfidfTable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #loadVocabList\n",
    "# vocabList = np.load('vocabList.npy')\n",
    "# vocabList[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #vectorize vocab\n",
    "# def vectorizeDict(weightDict):\n",
    "#     global vocabList\n",
    "    \n",
    "#     vector = []\n",
    "#     for vocab in vocabList:\n",
    "#         if vocab not in weightDict:\n",
    "#             vector.append(0.0)\n",
    "#         else:\n",
    "#             vector.append(weightDict[vocab])\n",
    "#     return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# tfidfDf = tfidfDf.rdd.map(lambda x: (x[0], vectorizeDict(x[1])) ).toDF().select(col(\"_1\").alias(\"index\"), col(\"_2\").alias(\"song\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #all the types of genre\n",
    "# genreDf = spark.read.csv('genre.csv').select(col(\"_c0\").alias(\"index\"), col(\"_c1\").alias(\"genre\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combinedDf = tfidfDf.join(genreDf, tfidfDf.index == genreDf.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
