{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark import SQLContext, SparkContext\n",
    "from collections import namedtuple\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType, Row, ArrayType, StringType\n",
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.mllib.feature import HashingTF, IDF\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import  HashingTF,Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"COMP 4651\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+-----+--------------------+--------------------+----+\n",
      "|         artist|genre|index|              lyrics|                song|year|\n",
      "+---------------+-----+-----+--------------------+--------------------+----+\n",
      "|beyonce-knowles|  Pop|    0|Oh baby, how you ...|           ego-remix|2009|\n",
      "|beyonce-knowles|  Pop|    1|playin' everythin...|        then-tell-me|2009|\n",
      "|beyonce-knowles|  Pop|    2|If you search\n",
      "For...|             honesty|2009|\n",
      "|beyonce-knowles|  Pop|    3|Oh oh oh I, oh oh...|     you-are-my-rock|2009|\n",
      "|beyonce-knowles|  Pop|    4|Party the people,...|       black-culture|2009|\n",
      "|beyonce-knowles|  Pop|    5|I heard\n",
      "Church be...|all-i-could-do-wa...|2009|\n",
      "|beyonce-knowles|  Pop|    6|This is just anot...|  once-in-a-lifetime|2009|\n",
      "|beyonce-knowles|  Pop|    7|Waiting, waiting,...|             waiting|2009|\n",
      "|beyonce-knowles|  Pop|    8|[Verse 1:]\n",
      "I read...|           slow-love|2009|\n",
      "|beyonce-knowles|  Pop|    9|N-n-now, honey\n",
      "Yo...|why-don-t-you-lov...|2009|\n",
      "|beyonce-knowles|  Pop|   10|I lay alone awake...|       save-the-hero|2009|\n",
      "|beyonce-knowles|  Pop|   11|Hello hello baby ...|           telephone|2009|\n",
      "|beyonce-knowles|  Pop|   12|Feels like I'm lo...|     ice-cream-truck|2009|\n",
      "|beyonce-knowles|  Pop|   13|Youre everything ...|no-broken-hearted...|2009|\n",
      "|beyonce-knowles|  Pop|   14|I gotta give up\n",
      "t...|             control|2009|\n",
      "|beyonce-knowles|  Pop|   15|It really hurts t...|       i-m-alone-now|2009|\n",
      "|beyonce-knowles|  Pop|   16|You're bad for me...|              poison|2009|\n",
      "|beyonce-knowles|  Pop|   17|[Chorus:]\n",
      "I'm a w...|    world-wide-women|2007|\n",
      "|beyonce-knowles|  Pop|   18|Ay\n",
      "Ay\n",
      "Ay (Nobody ...|      beautiful-liar|2007|\n",
      "|beyonce-knowles|  Pop|   19|Ay! Ay!\n",
      "(Nobody l...|beautiful-liar-sp...|2007|\n",
      "+---------------+-----+-----+--------------------+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sqlContext = SQLContext(sc)\n",
    "\n",
    "\n",
    "\n",
    "#compute the length of song\n",
    "size_ = udf(lambda xs: len(xs), IntegerType())\n",
    "\n",
    "#filter out null song value  note: filter == where, dunno why do they create two functions...\n",
    "raw_df = spark.read.json(\"data/lyrics.json\").where( (col('lyrics')).isNotNull() ).filter(size_(col('lyrics')) >0)\n",
    "\n",
    "# .na.drop(subset=[\"\"])\n",
    "# raw_df.select(raw_df['lyrics'], F.where((raw_df['index'] == 160))).show()\n",
    "raw_df\n",
    "raw_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+-----+--------------------+--------------------+----+-----+\n",
      "|         artist|genre|index|              lyrics|                song|year|label|\n",
      "+---------------+-----+-----+--------------------+--------------------+----+-----+\n",
      "|beyonce-knowles|  Pop|    0|Oh baby, how you ...|           ego-remix|2009|    6|\n",
      "|beyonce-knowles|  Pop|    1|playin' everythin...|        then-tell-me|2009|    6|\n",
      "|beyonce-knowles|  Pop|    2|If you search\n",
      "For...|             honesty|2009|    6|\n",
      "|beyonce-knowles|  Pop|    3|Oh oh oh I, oh oh...|     you-are-my-rock|2009|    6|\n",
      "|beyonce-knowles|  Pop|    4|Party the people,...|       black-culture|2009|    6|\n",
      "|beyonce-knowles|  Pop|    5|I heard\n",
      "Church be...|all-i-could-do-wa...|2009|    6|\n",
      "|beyonce-knowles|  Pop|    6|This is just anot...|  once-in-a-lifetime|2009|    6|\n",
      "|beyonce-knowles|  Pop|    7|Waiting, waiting,...|             waiting|2009|    6|\n",
      "|beyonce-knowles|  Pop|    8|[Verse 1:]\n",
      "I read...|           slow-love|2009|    6|\n",
      "|beyonce-knowles|  Pop|    9|N-n-now, honey\n",
      "Yo...|why-don-t-you-lov...|2009|    6|\n",
      "|beyonce-knowles|  Pop|   10|I lay alone awake...|       save-the-hero|2009|    6|\n",
      "|beyonce-knowles|  Pop|   11|Hello hello baby ...|           telephone|2009|    6|\n",
      "|beyonce-knowles|  Pop|   12|Feels like I'm lo...|     ice-cream-truck|2009|    6|\n",
      "|beyonce-knowles|  Pop|   13|Youre everything ...|no-broken-hearted...|2009|    6|\n",
      "|beyonce-knowles|  Pop|   14|I gotta give up\n",
      "t...|             control|2009|    6|\n",
      "|beyonce-knowles|  Pop|   15|It really hurts t...|       i-m-alone-now|2009|    6|\n",
      "|beyonce-knowles|  Pop|   16|You're bad for me...|              poison|2009|    6|\n",
      "|beyonce-knowles|  Pop|   17|[Chorus:]\n",
      "I'm a w...|    world-wide-women|2007|    6|\n",
      "|beyonce-knowles|  Pop|   18|Ay\n",
      "Ay\n",
      "Ay (Nobody ...|      beautiful-liar|2007|    6|\n",
      "|beyonce-knowles|  Pop|   19|Ay! Ay!\n",
      "(Nobody l...|beautiful-liar-sp...|2007|    6|\n",
      "+---------------+-----+-----+--------------------+--------------------+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# change genre to numeric\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "\n",
    "\n",
    "genreList = raw_df.select('genre').distinct().rdd.map(lambda x:x.genre).collect()\n",
    "genreList\n",
    "\n",
    "def changeToNumeric(genre):\n",
    "    return genreList.index(genre)\n",
    "\n",
    "udf_changeToNumeric = udf(changeToNumeric, IntegerType()) # if the function returns an int\n",
    "raw_df = raw_df.withColumn(\"label\", udf_changeToNumeric('genre'))\n",
    "raw_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n",
    "tokenizer = Tokenizer(inputCol=\"lyrics\", outputCol=\"tokenized_song\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\",numFeatures = 100)\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "raw_df = raw_df.select(col(\"index\"), col(\"lyrics\"), col('label'))\n",
    "# Fit the pipeline to training documents.\n",
    "\n",
    "#sample without replacement\n",
    "training_df = raw_df.sample(False,0.7)\n",
    "model = pipeline.fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4646639128802103"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction\n",
    "\n",
    "#sample without replacement\n",
    "validation_df = raw_df.sample(False,0.1)\n",
    "prediction = model.transform(validation_df)\n",
    "selected = prediction.select(\"index\", \"prediction\")\n",
    "combined_df = validation_df.select('index','label').join(selected, validation_df.index == selected.index)\n",
    "\n",
    "# for row in combined_df.collect():\n",
    "#     rid, prob, prediction = row\n",
    "#     print(\"(%d) --> prob=%s, prediction=%f\" % (rid, str(prob), prediction))\n",
    "    \n",
    "num_correct = combined_df.select( (combined_df.label == combined_df.prediction).alias('correct') ).rdd.map(lambda x: 1 if x.correct == True else 0).reduce(lambda a,b:a+b)\n",
    "accuracy = (num_correct + 0.0) / combined_df.count()\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+-----+--------------------+---------------+----+-------------+\n",
      "|         artist|genre|index|              lyrics|           song|year|numeric_genre|\n",
      "+---------------+-----+-----+--------------------+---------------+----+-------------+\n",
      "|beyonce-knowles|  Pop|    0|Oh baby, how you ...|      ego-remix|2009|            6|\n",
      "|beyonce-knowles|  Pop|    1|playin' everythin...|   then-tell-me|2009|            6|\n",
      "|beyonce-knowles|  Pop|    2|If you search\n",
      "For...|        honesty|2009|            6|\n",
      "|beyonce-knowles|  Pop|    3|Oh oh oh I, oh oh...|you-are-my-rock|2009|            6|\n",
      "+---------------+-----+-----+--------------------+---------------+----+-------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# split_regex = r'\\W+'\n",
    "# STOPWORDS_PATH = 'data/stopwords.txt'\n",
    "# stopwords = set(spark.read.text(STOPWORDS_PATH).rdd.map(lambda x: x.value).collect())\n",
    "# def tokenize(string):\n",
    "#     \"\"\" An implementation of input string tokenization that excludes stopwords\n",
    "#     Args:\n",
    "#         string (str): input string\n",
    "#     Returns:\n",
    "#         list: a list of tokens without stopwords\n",
    "#     \"\"\"\n",
    "    \n",
    "#     regexed = [ token.lower() for token in re.split(split_regex, string) if len(token)]\n",
    "#     return  [token for token in regexed if token not in stopwords]#get rid of empty stuff\n",
    "# udf_tokenize = udf(tokenize, ArrayType(StringType())) # if the function returns an int\n",
    "# raw_df = raw_df.withColumn(\"tokenized_song\", udf_changeToNumeric('lyrics'))\n",
    "# raw_df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # get rid of special character and split, then tokenize\n",
    "# raw_to_token = raw_df.select('index','lyrics').rdd.map(lambda x: (x[0],re.sub(r\"[^a-zA-Z0-9]+\", ' ', x[1]).split(\" \") ))\\\n",
    "# .map(lambda x: (x[0],  tokenize(' '.join(x[1])) ))\n",
    "# raw_to_token.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "transform() takes at least 2 arguments (1 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-445236a9f83c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mhashingTF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHashingTF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhashingTF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# While applying HashingTF only needs a single pass to the data, applying IDF needs two passes:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: transform() takes at least 2 arguments (1 given)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# hashingTF = HashingTF()\n",
    "# tf = hashingTF.transform()\n",
    "\n",
    "# # While applying HashingTF only needs a single pass to the data, applying IDF needs two passes:\n",
    "# # First to compute the IDF vector and second to scale the term frequencies by IDF.\n",
    "# tf.cache()\n",
    "# idf = IDF().fit(tf)\n",
    "# tfidf = idf.transform(tf)\n",
    "# #\n",
    "# # spark.mllib's IDF implementation provides an option for ignoring terms\n",
    "# # which occur in less than a minimum number of documents.\n",
    "# # In such cases, the IDF for these terms is set to 0.\n",
    "# # This feature can be used by passing the minDocFreq value to the IDF constructor.\n",
    "# idfIgnore = IDF(minDocFreq=2).fit(tf)\n",
    "# tfidfIgnore = idfIgnore.transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.read.parquet('tfidf.parquet').toDF('index','song').createOrReplaceTempView('tfidfTable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidfDf = spark.read.table('tfidfTable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([u'm', u'love', u'like', u'know', u're', u'll', u'oh', u'got',\n",
       "       u'get', u'one'],\n",
       "      dtype='<U16')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loadVocabList\n",
    "vocabList = np.load('vocabList.npy')\n",
    "vocabList[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#vectorize vocab\n",
    "def vectorizeDict(weightDict):\n",
    "    global vocabList\n",
    "    \n",
    "    vector = []\n",
    "    for vocab in vocabList:\n",
    "        if vocab not in weightDict:\n",
    "            vector.append(0.0)\n",
    "        else:\n",
    "            vector.append(weightDict[vocab])\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidfDf = tfidfDf.rdd.map(lambda x: (x[0], vectorizeDict(x[1])) ).toDF().select(col(\"_1\").alias(\"index\"), col(\"_2\").alias(\"song\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#all the types of genre\n",
    "genreDf = spark.read.csv('genre.csv').select(col(\"_c0\").alias(\"index\"), col(\"_c1\").alias(\"genre\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combinedDf = tfidfDf.join(genreDf, tfidfDf.index == genreDf.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "\n",
    "combinedDf.cache()\n",
    "parsedData = combinedDf.rdd.sample(False, 0.01).map(lambda x: LabeledPoint (x[2], x[1]))\n",
    "\n",
    "# Build the model\n",
    "model = LogisticRegressionWithLBFGS.train(parsedData)\n",
    "\n",
    "# # Evaluating the model on training data\n",
    "# labelsAndPreds = parsedData.map(lambda p: (p.label, model.predict(p.features)))\n",
    "# trainErr = labelsAndPreds.filter(lambda lp: lp[0] != lp[1]).count() / float(parsedData.count())\n",
    "# print(\"Training Error = \" + str(trainErr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
